{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityajain2407/aj-biz-cases/blob/main/AJ_Yulu_Review_HT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement:"
      ],
      "metadata": {
        "id": "emgc5Jymgwd8"
      },
      "id": "emgc5Jymgwd8"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Based on the concept of Demand forecasting - what are peak hours, when is the right time to charge, which season will be drop/peak to manage\n",
        "availabiliyt of bikes\n",
        "\n",
        "\n",
        "Select an appropriate test to check whether:\n",
        "Working Day has effect on number of electric cycles rented (count) - 2 sample independent t-test\n",
        "-cant be anova as need 2+ series- here we have only 2 1/0\n",
        "- no z-test as need std devi of population\n",
        "- cant be chi-square as need both to be cat- using proprtions --> test of independence - crosstab, proortions,\n",
        "- here it is cat and numerical (count)\n",
        "\n",
        "No. of cycles rented similar or different in different seasons - num and cat - more than 2 seasons - ANOVA\n",
        "No. of cycles rented similar or different in different weather - - more than 2 seasons - ANOVA\n",
        "Weather is dependent on season (check between 2 predictor variable) - both cat --> chi-square\n",
        "\n",
        "Assumtions for ANOVA\n",
        "1- check normality\n",
        "2- equal variance among each group\n",
        "\n",
        "even if assumptoins fail= still go ahead and try it out for now\n",
        "\n",
        "Whatever insights we get is for the sample data, but we need HT to understand if thats true for Population or not\n",
        "1. EDA\n",
        "2. HT\n",
        "\n",
        "Each row represents 1 hr of data\n",
        "object --> Datetime (not imp)\n",
        "No missing data\n",
        "no duplicates\n",
        "Sampling done is good - as for every season almost same no. of hours is given in data - no bias to any season\n",
        "But for weather it is uneven - there is outliser for weather 4 - can ignore this weatyher for analysis\n",
        "As cat are also given numbers (encoded), we can check relationship between all using heatmap\n",
        "extra- to capture non-linear relaitonship between vartianbles --> using spearmen correaltion (default - pearson works for liner relaitonship - Google)\n",
        "Target variable is count - need to check what vaariables impacts it --> temp, humidity\n",
        "But as working day, holiday ,etc are not giving exact intuition and its a grey area - we can check bivariate analysis to get more insights\n",
        "Why to chose median over mean? Outlier robust eg\n",
        "We can confirm the correaltion suing box plot - as median of diff seasons are close - not much effect\n",
        "but for weather 3, its diff from rest - so some impact\n",
        "\n",
        "We can see there are outlisers - not a good idea to direclty remove them - as not aware of the reasoning - is it due to\n",
        "data collection error, data sampling error or natrual outliers. If natrual - removing it will be like manipulating the data - incorrect\n",
        "so we will transform it to Normal -- why - bcoz most of the tests assume distribution to be normal (parametric tests) -\n",
        "extra 0- log transform is reversible - we can get back original data -we are not permanently changing it\n",
        "\n",
        "\n",
        "Testing for normality was not reqd - as we can asusme and go ahead with analysis\n",
        "\n",
        "From IQR - we can see how many outlisers and as 300 which is less % of overall, if we knew the reason and not natural out;lsers\n",
        "we can easlity reamove the outlisers\n",
        "\n",
        "So far, we have done EDA on sample - for now, season weather, temp and humidity has impact,\n",
        "workingday, holiday has no impact --<\n",
        "\n",
        "Now, lets do HT over it\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hC5FZ7EF-Ji",
        "outputId": "e0a5411e-5962-458e-d349-5c8c25c68bdc"
      },
      "id": "5hC5FZ7EF-Ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBased on the concept of Demand forecasting - what are peak hours, when is the right time to charge, which season will be drop/peak to manage\\navailabiliyt of bikes\\n\\n\\nSelect an appropriate test to check whether: \\nWorking Day has effect on number of electric cycles rented (count) - 2 sample independent t-test\\n-cant be anova as need 2+ series- here we have only 2 1/0 \\n- no z-test as need std devi of population\\n- cant be chi-square as need both to be cat- using proprtions --> test of independence - crosstab, proortions, \\n- here it is cat and numerical (count)\\n\\nNo. of cycles rented similar or different in different seasons - num and cat - more than 2 seasons - ANOVA\\nNo. of cycles rented similar or different in different weather - - more than 2 seasons - ANOVA\\nWeather is dependent on season (check between 2 predictor variable) - both cat --> chi-square\\n\\nAssumtions for ANOVA\\n1- check normality \\n2- equal variance among each group\\n\\neven if assumptoins fail= still go ahead and try it out for now\\n\\nWhatever insights we get is for the sample data, but we need HT to understand if thats true for Population or not\\n1. EDA\\n2. HT\\n\\nEach row represents 1 hr of data\\nobject --> Datetime (not imp)\\nNo missing data\\nno duplicates\\nSampling done is good - as for every season almost same no. of hours is given in data - no bias to any season\\nBut for weather it is uneven - there is outliser for weather 4 - can ignore this weatyher for analysis\\nAs cat are also given numbers (encoded), we can check relationship between all using heatmap\\nextra- to capture non-linear relaitonship between vartianbles --> using spearmen correaltion (default - pearson works for liner relaitonship - Google)\\nTarget variable is count - need to check what vaariables impacts it --> temp, humidity\\nBut as working day, holiday ,etc are not giving exact intuition and its a grey area - we can check bivariate analysis to get more insights\\nWhy to chose median over mean? Outlier robust eg\\nWe can confirm the correaltion suing box plot - as median of diff seasons are close - not much effect\\nbut for weather 3, its diff from rest - so some impact\\n\\nWe can see there are outlisers - not a good idea to direclty remove them - as not aware of the reasoning - is it due to \\ndata collection error, data sampling error or natrual outliers. If natrual - removing it will be like manipulating the data - incorrect\\nso we will transform it to Normal -- why - bcoz most of the tests assume distribution to be normal (parametric tests) -\\nextra 0- log transform is reversible - we can get back original data -we are not permanently changing it\\n\\n\\nTesting for normality was not reqd - as we can asusme and go ahead with analysis\\n\\nFrom IQR - we can see how many outlisers and as 300 which is less % of overall, if we knew the reason and not natural out;lsers\\nwe can easlity reamove the outlisers\\n\\nSo far, we have done EDA on sample - for now, season weather, temp and humidity has impact, \\nworkingday, holiday has no impact --<\\n\\nNow, lets do HT over it\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries -\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sbn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind # T-test for independent samples\n",
        "from scipy.stats import shapiro # Shapiro-Wilkâ€™s test for Normality\n",
        "from scipy.stats import levene # Levene's test for Equality of Variance\n",
        "from scipy.stats import f_oneway # One-way ANOVA\n",
        "from scipy.stats import chi2_contingency # Chi-square test of independence\n",
        "# from scipy.stats import kstest # KS test for Normality"
      ],
      "metadata": {
        "id": "4W2K3olTgx10"
      },
      "id": "4W2K3olTgx10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "TvcaNytt7W_L"
      },
      "id": "TvcaNytt7W_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/file/d/1o94fXnmvrx6jRgI6S-SeZ3tfnKjCDY0i/view?usp=sharing'\n",
        "\n",
        "id = link.split(\"/\")[-2]\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('bike_sharing.csv')"
      ],
      "metadata": {
        "id": "HAxlEzIIUmST",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "25b41275-aaa9-4281-ad43-784609d3c252"
      },
      "id": "HAxlEzIIUmST",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2bf50350e89a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdownloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdownloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bike_sharing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c011fa5",
      "metadata": {
        "id": "3c011fa5"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset -\n",
        "df=pd.read_csv('bike_sharing.csv')\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the dataset -\n",
        "print(\"No. of rows : \", df.shape[0])"
      ],
      "metadata": {
        "id": "WO2Y15xQZLzh"
      },
      "id": "WO2Y15xQZLzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0e522d",
      "metadata": {
        "id": "aa0e522d"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values -\n",
        "(df.isna().sum() / len(df)) * 100"
      ],
      "metadata": {
        "id": "VjYPY00pa-bM"
      },
      "id": "VjYPY00pa-bM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate rows -\n",
        "dup_rows = df[df.duplicated()]\n",
        "print(\"No. of duplicate rows: \", dup_rows.shape[0])"
      ],
      "metadata": {
        "id": "KryzxlCrf_2S"
      },
      "id": "KryzxlCrf_2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a74ed3",
      "metadata": {
        "id": "02a74ed3"
      },
      "outputs": [],
      "source": [
        "def dist_check(df, col_name):\n",
        "  print(\"Unique values : \", df[col_name].unique())\n",
        "  print(\"Value counts : \")\n",
        "  print(df[col_name].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_list = ['workingday',\t'holiday',\t'weather', 'season']\n",
        "for col in col_list:\n",
        "  print(col, \" -\")\n",
        "  dist_check(df, col)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "FwUHmL1MVYr_"
      },
      "id": "FwUHmL1MVYr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap -\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sbn.heatmap(df.corr(method='spearman'),\n",
        "            annot=True, cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pHW6XwotxmlK"
      },
      "id": "pHW6XwotxmlK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the correlation we can verify some logical points:\n",
        "- feeling temperature or aparent temprature and temp are highly correlated, because they are most of the times approximately the same have a very small diffrerence\n",
        "- count, causal, registered are all correlated to each other because all of them"
      ],
      "metadata": {
        "id": "x7bWGa8kxS60"
      },
      "id": "x7bWGa8kxS60"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping highly correlated columns -\n",
        "dfn = df.drop(columns=['casual', 'registered', 'atemp'])"
      ],
      "metadata": {
        "id": "7dNduYEbxpd0"
      },
      "id": "7dNduYEbxpd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Detection using Boxplots -\n",
        "\n",
        "sbn.set(style=\"whitegrid\")\n",
        "fig = plt.figure(figsize=(8, 25))\n",
        "fig.subplots_adjust(right=1.5)\n",
        "\n",
        "for plot in range(1, len(col_list)+1):\n",
        "    plt.subplot(5, 2, plot)\n",
        "    sbn.boxplot(x=dfn[col_list[plot-1]], y=dfn['count'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8m6UUHFcuZni"
      },
      "id": "8m6UUHFcuZni",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of 'count' column -\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "#Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "sbn.distplot(dfn['count'], bins=10)\n",
        "\n",
        "#Boxplot\n",
        "plt.subplot(1, 2, 2)\n",
        "sbn.boxplot(y=dfn['count'])\n",
        "plt.title('Boxplot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W2wh_RI_yUZa"
      },
      "id": "W2wh_RI_yUZa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that outliers are present in the given columns. We need to figure out a way to deal with them before starting with the tests."
      ],
      "metadata": {
        "id": "wQO3FK6frbSk"
      },
      "id": "wQO3FK6frbSk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have multiple options available on how to proceed with these outlier values.\n",
        "\n",
        "1. Try to understand if these values make any sense according to the business problem. If yes, then we can keep them as it is.\n",
        "2. In case these outliers are some invalid values which do not make much sense, we can remove them using the IQR.\n",
        "3. Or we can apply a log transformation on the data to reduce the effect of these outliers."
      ],
      "metadata": {
        "id": "kjAUen1e0ajV"
      },
      "id": "kjAUen1e0ajV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. -\n",
        "* The outliers in the given data set are the no. of bike rides per session/day. These values could sometimes be higher than expected due to increase in the crowd on certain days/occasions.\n",
        "* These data values are important for capturing variations in the data. Hence, in this case, the ideal approach of dealing with outliers would be to leave them as it is.\n",
        "* But since the tests that we are going to apply are based on the assumption that the dataset is normal or near normal, we will drop those outlier values using the IQR method."
      ],
      "metadata": {
        "id": "MBIoDwNFnCEi"
      },
      "id": "MBIoDwNFnCEi"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "# Checking distribution after applying log transformation -\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "#Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "sbn.distplot(np.log(dfn['count']), bins=10)\n",
        "\n",
        "#Boxplot\n",
        "plt.subplot(1, 2, 2)\n",
        "sbn.boxplot(y=np.log(dfn['count']))\n",
        "plt.title('Boxplot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lHlgzgDT19zM"
      },
      "id": "lHlgzgDT19zM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c19d8e",
      "metadata": {
        "id": "c7c19d8e"
      },
      "outputs": [],
      "source": [
        "# # 3.\n",
        "# # Outlier Treatment using IQR (not needed but, we can do it) -\n",
        "\n",
        "# q1 = dfn['count'].quantile(0.25)\n",
        "# q3 = dfn['count'].quantile(0.75)\n",
        "# iqr = q3-q1\n",
        "\n",
        "# dfn = dfn[(dfn['count']>(q1-1.5*iqr) ) & (dfn['count']<(q3+1.5*iqr))]\n",
        "\n",
        "# print(\"No. of rows : \", dfn.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggregating the total no. of bike rides based on the given factors -"
      ],
      "metadata": {
        "id": "axA5s7YUYTDR"
      },
      "id": "axA5s7YUYTDR"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Workingday -\n",
        "pd.DataFrame(dfn.groupby('workingday')['count'].describe())"
      ],
      "metadata": {
        "id": "olrYnZaFYTta"
      },
      "id": "olrYnZaFYTta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Holiday -\n",
        "pd.DataFrame(dfn.groupby('holiday')['count'].describe())"
      ],
      "metadata": {
        "id": "rKVDP5PpYTyT"
      },
      "id": "rKVDP5PpYTyT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Season -\n",
        "pd.DataFrame(dfn.groupby('season')['count'].describe())"
      ],
      "metadata": {
        "id": "YQEK55YNYT3P"
      },
      "id": "YQEK55YNYT3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Weather -\n",
        "pd.DataFrame(dfn.groupby('weather')['count'].describe())"
      ],
      "metadata": {
        "id": "egDRLSrwYT-n"
      },
      "id": "egDRLSrwYT-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we ask ourselves such doubts in industry to get direction of analyssi\n",
        "#can do independent t-test : we are doing one tailed\n",
        "'''\n",
        "Extra - we no. of data points are large ? t test converges to z-test\n",
        "\n",
        "Most of the statistical packages  by default does t-test only\n",
        "'''"
      ],
      "metadata": {
        "id": "C5FoqbujPm6e"
      },
      "id": "C5FoqbujPm6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Ques. 1 - Is there any significant difference between the no. of bike rides on weekdays and weekends?\n",
        "---"
      ],
      "metadata": {
        "id": "cFoUBGA0b7GA"
      },
      "id": "cFoUBGA0b7GA"
    },
    {
      "cell_type": "markdown",
      "id": "ec8a180e",
      "metadata": {
        "id": "ec8a180e"
      },
      "source": [
        "#### **Step 1:** Define the null and alternate hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec8d21e",
      "metadata": {
        "id": "6ec8d21e"
      },
      "source": [
        "$H_0:$ The demand of bikes on weekdays is greater or similar to the demand of bikes on weekend.\n",
        "\n",
        "$H_a:$ The demand of bikes on weekdays is less than the demand of bikes on weekend.\n",
        "\n",
        "Let $\\mu_1$ and $\\mu_2$ be the average no. of bikes rented on weekdays and weekends respectively.\n",
        "\n",
        "Mathematically, the above formulated hypothesis can be written as:\n",
        "\n",
        "$H_0: \\mu_1 >= \\mu_2$\n",
        "\n",
        "$H_a: \\mu_1 < \\mu_2$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques.** What is the difference between a t-test and a z-test?\n",
        "\n",
        "**Ans.**\n",
        "\n",
        "* A t-test looks at two sets of data that are different from each other, with no standard deviation or variance.\n",
        "\n",
        "* A z-test views the averages of data sets that are different from each other but have the standard deviation or variance given.\n",
        "\n",
        "* The t test as compared with z test has its advantage for small sample comparison. As n increases, t approaches to z. The advantage of t test disappears, and t distribution simply becomes z distribution.\n",
        "\n",
        "* In other words, with large n, t test is just close to z test and one doen't lose anything to continue to use t test.\n",
        "\n",
        "* In the past, for convenience, we use z table when n > 30.  We don't have to do it anymore.\n",
        "\n",
        "* In fact, all statistical packages use t test even n is large. This is easy, convenience with computer programming, and is correct. All statistical packages are good references."
      ],
      "metadata": {
        "id": "mZmivAhd2Lnf"
      },
      "id": "mZmivAhd2Lnf"
    },
    {
      "cell_type": "markdown",
      "id": "87c763c6",
      "metadata": {
        "id": "87c763c6"
      },
      "source": [
        "#### **Step 2:** Select an appropriate test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the standard deviation of the population is not known."
      ],
      "metadata": {
        "id": "iEQ3Hu2I6ISA"
      },
      "id": "iEQ3Hu2I6ISA"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "we keep equal number to make sure we dont introduce variance - follow this practice"
      ],
      "metadata": {
        "id": "MvI4-433uFfx"
      },
      "id": "MvI4-433uFfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekday = dfn[dfn['workingday'] == 1]['count'].sample(2999)\n",
        "weekend = dfn[dfn['workingday'] == 0]['count'].sample(2999)"
      ],
      "metadata": {
        "id": "HmiUrzwlqOMq"
      },
      "id": "HmiUrzwlqOMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques.** Why do we take same no. of samples from two different populations for conducting the tests?\n",
        "\n",
        "**Ans.**\n",
        "* Unequal sample sizes can lead to unequal variances between samples, which affects the assumption of equal variances in tests like t-test, ANOVA, etc.\n",
        "\n",
        "* Having both unequal sample sizes and variances dramatically affects the statistical power of a test.\n"
      ],
      "metadata": {
        "id": "mMIOmYYk2QPq"
      },
      "id": "mMIOmYYk2QPq"
    },
    {
      "cell_type": "code",
      "source": [
        "print('The sample standard deviation of the bike rides on weekday is:', round(weekday.std(), 2))\n",
        "print('The sample standard deviation of the bike rides on weekend is:', round(weekend.std(), 2))\n",
        "#unequal variance"
      ],
      "metadata": {
        "id": "PJ5u9yhqqPdC"
      },
      "id": "PJ5u9yhqqPdC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the sample standard deviations are different, the population standard deviations can be assumed to be different."
      ],
      "metadata": {
        "id": "aK4T7blnqWZO"
      },
      "id": "aK4T7blnqWZO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a one-tailed test concerning two population means from two independent populations. As the population standard deviations are unknown, the two sample independent t-test will be the appropriate test for this problem."
      ],
      "metadata": {
        "id": "W9xL-K0SqMl-"
      },
      "id": "W9xL-K0SqMl-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Decide the significance level\n",
        "\n",
        "As given in the problem statement, we select Î± = 0.05."
      ],
      "metadata": {
        "id": "aJF22wqXfIXH"
      },
      "id": "aJF22wqXfIXH"
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05"
      ],
      "metadata": {
        "id": "TQ-dOfezfJCz"
      },
      "id": "TQ-dOfezfJCz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c0c7304",
      "metadata": {
        "id": "4c0c7304"
      },
      "source": [
        "#### **Step 4:** Calculate the p-value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def result(p_value, alpha):\n",
        "  if p_value < alpha:\n",
        "    print(f'As the p-value {p_value} is less than the level of significance, we reject the null hypothesis.')\n",
        "  else:\n",
        "    print(f'As the p-value {p_value} is greater than the level of significance, we fail to reject the null hypothesis.')"
      ],
      "metadata": {
        "id": "pePsO9p7cVBY"
      },
      "id": "pePsO9p7cVBY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10379138",
      "metadata": {
        "id": "10379138"
      },
      "outputs": [],
      "source": [
        "test_stat, p_value = ttest_ind(weekday, weekend, equal_var=False, alternative='less') #two-tailed, greater - extra -\n",
        "print('The p-value is : ', p_value)\n",
        "#you can get diff p value depending on teh pre-processing done\n",
        "result(p_value, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Since the p-value is greater than the 5% significance level, we fail to reject the null hypothesis. Hence, we have enough statistical evidence to say that the average no. of bike rides during weekdays is greater than or equal to those on weekends."
      ],
      "metadata": {
        "id": "a6DkwuNoeRR_"
      },
      "id": "a6DkwuNoeRR_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Ques. 2 - Is there any significant difference between the no. of bike rides on regular days and holidays?\n",
        "---"
      ],
      "metadata": {
        "id": "6U7Lb36rTJ99"
      },
      "id": "6U7Lb36rTJ99"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1:** Define the null and alternate hypothesis"
      ],
      "metadata": {
        "id": "t2iJwXeUTka6"
      },
      "id": "t2iJwXeUTka6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$H_0:$ The demand of bikes on regular days is greater or similar to the demand of bikes on holidays.\n",
        "\n",
        "$H_a:$ The demand of bikes on regular days is less than the demand of bikes on holidays.\n",
        "\n",
        "Let $\\mu_1$ and $\\mu_2$ be the average no. of bikes rented on regular days and holidays respectively.\n",
        "\n",
        "Mathematically, the above formulated hypothesis can be written as:\n",
        "\n",
        "$H_0: \\mu_1 >= \\mu_2$\n",
        "\n",
        "$H_a: \\mu_1 < \\mu_2$"
      ],
      "metadata": {
        "id": "z73zbtB9Tlem"
      },
      "id": "z73zbtB9Tlem"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2:** Select an appropriate test"
      ],
      "metadata": {
        "id": "0MWvhnB7UnjR"
      },
      "id": "0MWvhnB7UnjR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the standard deviation of the population is not known."
      ],
      "metadata": {
        "id": "NUGqJyN5VZvc"
      },
      "id": "NUGqJyN5VZvc"
    },
    {
      "cell_type": "code",
      "source": [
        "holiday = dfn[dfn['holiday'] == 1]['count'].sample(299)\n",
        "regular = dfn[dfn['holiday'] == 0]['count'].sample(299)"
      ],
      "metadata": {
        "id": "G9seigrXUqvM"
      },
      "id": "G9seigrXUqvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The sample standard deviation of the bike rides on holidays is:', round(holiday.std(), 2))\n",
        "print('The sample standard deviation of the bike rides on regular days is:', round(regular.std(), 2))"
      ],
      "metadata": {
        "id": "Biv0fzvmUqyJ"
      },
      "id": "Biv0fzvmUqyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the sample standard deviations are different, the population standard deviations can be assumed to be different."
      ],
      "metadata": {
        "id": "F0b02jYOcPsO"
      },
      "id": "F0b02jYOcPsO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is also a one-tailed test concerning two population means from two independent populations. As the population standard deviations are unknown, the two sample independent t-test will be the appropriate test for this problem."
      ],
      "metadata": {
        "id": "Vd6CYctsVhKM"
      },
      "id": "Vd6CYctsVhKM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Decide the significance level\n",
        "\n",
        "The significance level (Î±) is already set to 5% i.e., 0.05"
      ],
      "metadata": {
        "id": "paIq-F6PUv3B"
      },
      "id": "paIq-F6PUv3B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4:** Calculate the p-value"
      ],
      "metadata": {
        "id": "iJoofYptU0IY"
      },
      "id": "iJoofYptU0IY"
    },
    {
      "cell_type": "code",
      "source": [
        "test_stat, p_value = ttest_ind(regular, holiday, equal_var=False, alternative='less')\n",
        "print('The p-value is : ', p_value)\n",
        "\n",
        "result(p_value, alpha)"
      ],
      "metadata": {
        "id": "ZQtqer2dVHz1"
      },
      "id": "ZQtqer2dVHz1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Since the p-value is greater than the 5% significance level, we fail to reject the null hypothesis. Hence, we have enough statistical evidence to say that the average no. of bike rides during regular days is greater than or equal to those on holidays."
      ],
      "metadata": {
        "id": "RWkrLRZGVGNV"
      },
      "id": "RWkrLRZGVGNV"
    },
    {
      "cell_type": "markdown",
      "id": "afc32419",
      "metadata": {
        "id": "afc32419"
      },
      "source": [
        "---\n",
        "### Ques. 3 - Is the demand of bicycles on rent same for different weather conditions?\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1:** Define the null and alternate hypothesis\n",
        "\n",
        "$H_0:$ The average no. of bike rides in different weather conditions  are equal.\n",
        "\n",
        "$H_a:$ The average no. of bike rides in different weather conditions are not equal.\n",
        "\n",
        "Let $\\mu_1$ and $\\mu_2$ be the average no. of bikes rented on weekdays and weekends respectively."
      ],
      "metadata": {
        "id": "wq3h-j2OoSNv"
      },
      "id": "wq3h-j2OoSNv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2:** Select an appropriate test\n"
      ],
      "metadata": {
        "id": "et3oo04WiFxf"
      },
      "id": "et3oo04WiFxf"
    },
    {
      "cell_type": "code",
      "source": [
        "dfn = dfn[~(dfn['weather']==4)] #due to outlier above"
      ],
      "metadata": {
        "id": "t5UVEa09qmkL"
      },
      "id": "t5UVEa09qmkL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to check if all 3 series are close or not - why 750 - because if do descibe 850 in w3 so need to be between 30 and 850\n",
        "#we will do ANOVA - now need to check normality and populaiotn variaance of all group equal - need to vlaidate assumptions\n",
        "#even if normlaity fails, we can still go ahead but variance thing should be true\n",
        "w1 = dfn[dfn['weather'] == 1]['count'].sample(750)\n",
        "w2 = dfn[dfn['weather'] == 2]['count'].sample(750)\n",
        "w3 = dfn[dfn['weather'] == 3]['count'].sample(750)"
      ],
      "metadata": {
        "id": "bYgfcceQqmnJ"
      },
      "id": "bYgfcceQqmnJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfn.groupby(['weather'])['count'].describe()"
      ],
      "metadata": {
        "id": "Mwtl1NnGqmqb"
      },
      "id": "Mwtl1NnGqmqb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a problem, concerning three independent population means. **One-way ANOVA** could be the appropriate test here provided normality and equality of variance assumptions are verified.\n",
        "\n",
        "The ANOVA test has important assumptions that must be satisfied in order for the associated p-value to be valid.\n",
        "\n",
        "* The samples are independent.\n",
        "* Each sample is from a normally distributed population.\n",
        "* The population variance of the groups are all equal."
      ],
      "metadata": {
        "id": "0ZpM-zPrqkn_"
      },
      "id": "0ZpM-zPrqkn_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will be using the following statistical tests to check the normality and euality of variance of the data set -\n",
        "\n",
        "* For testing of normality, Shapiro-Wilkâ€™s test is applied to the response variable.\n",
        "\n",
        "* For equality of variance, Levene test is applied to the response variable."
      ],
      "metadata": {
        "id": "-TKFNtPiVXj9"
      },
      "id": "-TKFNtPiVXj9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shapiro-Wilkâ€™s test -\n",
        "\n",
        "We will test the null hypothesis\n",
        "\n",
        ">$H_0:$ Count follows normal distribution\n",
        "\n",
        "against the alternative hypothesis\n",
        "\n",
        ">$H_a:$ Count doesn't follow normal distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "PaLgpKEliJIx"
      },
      "id": "PaLgpKEliJIx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06baf969",
      "metadata": {
        "id": "06baf969"
      },
      "outputs": [],
      "source": [
        "# Assumption 1: Normality\n",
        "\n",
        "w, p_value = shapiro(dfn['count'].sample(4999))\n",
        "print('The p-value is : ', p_value)\n",
        "\n",
        "result(p_value, alpha)\n",
        "#not normal - but still as lots of data points - by CLT - we go ahead with assumption of normal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138cf2ed",
      "metadata": {
        "id": "138cf2ed"
      },
      "source": [
        "#### Leveneâ€™s test -\n",
        "\n",
        "We will test the null hypothesis\n",
        "\n",
        ">$H_0$: All the count variances are equal\n",
        "\n",
        "against the alternative hypothesis\n",
        "\n",
        ">$H_a$: At least one variance is different from the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2549eea2",
      "metadata": {
        "id": "2549eea2"
      },
      "outputs": [],
      "source": [
        "#Assumption 2: Homogeneity of Variance - this test is to check if gorups have equal variance or not\n",
        "\n",
        "stat, p_value = levene(w1, w2, w3)\n",
        "print('The p-value is : ', p_value)\n",
        "\n",
        "result(p_value, alpha)\n",
        "\n",
        "#we can also check from describe std data - so we cant apply ANOVA\n",
        "#HW try Kruskal-Wallis H-test\n",
        "\n",
        "#Extra: Parametyric - we assume distri to be normal to apply them, non-parametric - no such requirement of normality\n",
        "#we use parametric first as if its normal we get great insights - if doenst work, we go for non-parametric as they are more robust\n",
        "#due to no assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cheat sheet for all tests: https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
      ],
      "metadata": {
        "id": "cC8DnadvEeCF"
      },
      "id": "cC8DnadvEeCF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** If these assumptions are not true for a given set of data (like in this case), it may still be possible to use the **Kruskal-Wallis H-test** or the **Alexander-Govern test** although with some loss of power."
      ],
      "metadata": {
        "id": "4QSG7yPNh0cL"
      },
      "id": "4QSG7yPNh0cL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Central Limit Theorem -\n",
        "\n",
        "You all must have studies about the CLT in previous classes.\n",
        "\n",
        "* According to this theorem, the distribution of sample means approximates a normal distribution as the sample size gets larger, regardless of the population's distribution.\n",
        "\n",
        "* In other words, if we find the mean of a large number of independent random variables, the mean\n",
        "will follow a normal distribution, irrespective of the distribution of the original variables.\n",
        "\n",
        "* In practice, sample sizes equal to or greater than 30-40 are often considered sufficient for the CLT to hold.\n",
        "\n",
        "Hence, the sample size being large enough, we don't need to worry about the non-normality of distribution of the data set in hand before applying the tests.\n",
        "\n",
        "Eventually, as the sample size gets larger, the distribution of sample means will fall into a normal or near normal shape.\n"
      ],
      "metadata": {
        "id": "f9vCWvfh2YrU"
      },
      "id": "f9vCWvfh2YrU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques.** What are some of the basic methods (other than statistical tests) to test the normality & homogeneity of variance?"
      ],
      "metadata": {
        "id": "tPsIGkSEVu0B"
      },
      "id": "tPsIGkSEVu0B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. To check for **Normality -**"
      ],
      "metadata": {
        "id": "ZYxTuogKZ7Gd"
      },
      "id": "ZYxTuogKZ7Gd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Using a Histogram\n",
        "# You should see a 'Bell' shaped curve.\n",
        "sbn.distplot(dfn['count'].sample(4999))"
      ],
      "metadata": {
        "id": "cFjC-0R_WBBs"
      },
      "id": "cFjC-0R_WBBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Using a Q-Q plot\n",
        "# The linearity of points suggests that the data is normally distributed.\n",
        "stats.probplot(dfn['count'], dist='norm', fit=True, plot=plt)\n",
        "plt.title('Q-Q Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6hnqzkfmXpE8"
      },
      "id": "6hnqzkfmXpE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: Check skewness & kurtosis\n",
        "# Skewness should be close to 0 and Kurtosis close to 3.\n",
        "\n",
        "print(\"Skewness : \", df['count'].skew())\n",
        "print(\"Kurtosis : \", df['count'].kurt())"
      ],
      "metadata": {
        "id": "_4BOl6Or6EFf"
      },
      "id": "_4BOl6Or6EFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Method 4: Using KS Test to compare with the Gaussian CDF\n",
        "zs = (dfn[\"count\"] - dfn[\"count\"].mean())/dfn[\"count\"].std()\n",
        "stats.kstest(zs, stats.norm.cdf)"
      ],
      "metadata": {
        "id": "_EMVWVJN25UJ"
      },
      "id": "_EMVWVJN25UJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. To check for **Homogeneity of Variance -**"
      ],
      "metadata": {
        "id": "IGn2uWV6Z_hN"
      },
      "id": "IGn2uWV6Z_hN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1:\n",
        "print(w1.var(), w2.var(), w3.var())"
      ],
      "metadata": {
        "id": "fMmCcJQe2F27"
      },
      "id": "fMmCcJQe2F27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Decide the significance level\n",
        "\n",
        "The significance level (Î±) is already set to 5% i.e., 0.05"
      ],
      "metadata": {
        "id": "iv4RTCPPdrFM"
      },
      "id": "iv4RTCPPdrFM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4:** Calculate the p-value"
      ],
      "metadata": {
        "id": "e2UpK8Njd-wW"
      },
      "id": "e2UpK8Njd-wW"
    },
    {
      "cell_type": "markdown",
      "id": "e75a3971",
      "metadata": {
        "id": "e75a3971"
      },
      "source": [
        "One-way ANOVA -\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4ee3b1",
      "metadata": {
        "id": "7d4ee3b1"
      },
      "outputs": [],
      "source": [
        "test_stat, p_value = f_oneway(w1, w2, w3)\n",
        "print('The p-value is : ', p_value)\n",
        "\n",
        "result(p_value, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Since the p-value is less than the 5% significance level, we reject the null hypothesis. Hence, we have enough statistical evidence to say that the average no. of bike rides in different weather conditions are not equal."
      ],
      "metadata": {
        "id": "QPun4uGl6oUm"
      },
      "id": "QPun4uGl6oUm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Ques. 4 - Is the demand of bicycles on rent same for different seasons?\n",
        "---"
      ],
      "metadata": {
        "id": "1SXayIkB7xJC"
      },
      "id": "1SXayIkB7xJC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1:** Define the null and alternate hypothesis\n",
        "\n",
        "$H_0:$ The average no. of bike rides in different seasons  are equal.\n",
        "\n",
        "$H_a:$ The average no. of bike rides in different seasons are not equal."
      ],
      "metadata": {
        "id": "z5AYFH4o70iF"
      },
      "id": "z5AYFH4o70iF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2:** Select an appropriate test"
      ],
      "metadata": {
        "id": "va12TtijWj5c"
      },
      "id": "va12TtijWj5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e38889",
      "metadata": {
        "id": "f9e38889"
      },
      "outputs": [],
      "source": [
        "s1 = dfn[dfn['season'] == 1]['count'].sample(2399)\n",
        "s2 = dfn[dfn['season'] == 2]['count'].sample(2399)\n",
        "s3 = dfn[dfn['season'] == 3]['count'].sample(2399)\n",
        "s4 = dfn[dfn['season'] == 3]['count'].sample(2399)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfn.groupby(['season'])['count'].describe()"
      ],
      "metadata": {
        "id": "RiwyjP_HcRc-"
      },
      "id": "RiwyjP_HcRc-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Decide the significance level\n",
        "\n",
        "The significance level (Î±) is already set to 5% i.e., 0.05"
      ],
      "metadata": {
        "id": "YRSX_VZwd38M"
      },
      "id": "YRSX_VZwd38M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4:** Calculate the p-value"
      ],
      "metadata": {
        "id": "KuzMqpQOeEJN"
      },
      "id": "KuzMqpQOeEJN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already performed tests for normality and homogeneity of variance. So we will be directly moving onto the One-way ANOVA test."
      ],
      "metadata": {
        "id": "lP3zlNPBcrFe"
      },
      "id": "lP3zlNPBcrFe"
    },
    {
      "cell_type": "code",
      "source": [
        "test_stat, p_value = f_oneway(s1, s2, s3, s4)\n",
        "print('The p-value is : ', p_value)\n",
        "\n",
        "result(p_value, alpha)"
      ],
      "metadata": {
        "id": "Slorvd1nca3s"
      },
      "id": "Slorvd1nca3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Since the p-value is less than the 5% significance level, we reject the null hypothesis. Hence, we have enough statistical evidence to say that the average no. of bike rides in different seasons are not equal."
      ],
      "metadata": {
        "id": "oSKIgBd2dOlF"
      },
      "id": "oSKIgBd2dOlF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques.** How does the increase in sample size affect hypothesis testing?\n",
        "\n",
        "**Ans.** Increasing sample size makes the hypothesis test more sensitive, more likely to reject the null hypothesis when it is, in fact, false. Thus, it increases the power of the test."
      ],
      "metadata": {
        "id": "dgw9PRdN2nO1"
      },
      "id": "dgw9PRdN2nO1"
    },
    {
      "cell_type": "code",
      "source": [
        "#checking dependcy of 1 cat variable over another - chi-square test of independence - non-parametric - no requirement for normal distri"
      ],
      "metadata": {
        "id": "gPx2XSslKFNl"
      },
      "id": "gPx2XSslKFNl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Ques. 5 -  Are the weather conditions significantly different during different seasons?\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Hlpw6E1Ax4DF"
      },
      "id": "Hlpw6E1Ax4DF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1:** Define the null and alternate hypothesis\n",
        "\n",
        "$H_0:$ Weather conditions are independent of the season.\n",
        "\n",
        "$H_a:$ Weather condition depends on the ongoing season."
      ],
      "metadata": {
        "id": "QUUxs46oyU-0"
      },
      "id": "QUUxs46oyU-0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2:** Select an appropriate test"
      ],
      "metadata": {
        "id": "WRO22nl1yp8P"
      },
      "id": "WRO22nl1yp8P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the data values in 'season' and 'weather' columns are numerical, as per our intuition, they still represent different catgories. Hence, we will encode them accordingly before moving onto the tests."
      ],
      "metadata": {
        "id": "4gTA8q9zytYx"
      },
      "id": "4gTA8q9zytYx"
    },
    {
      "cell_type": "code",
      "source": [
        "dict1 = {1: 'Sunny',\n",
        "         2: 'Cloudy',\n",
        "         3: 'Rainy'}\n",
        "dfn['weather_enc'] = dfn['weather'].map(dict1)"
      ],
      "metadata": {
        "id": "AhDiAHS_dGKo"
      },
      "id": "AhDiAHS_dGKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict2 = {1: 'Summer',\n",
        "         2: 'Monsoon',\n",
        "         3: 'Winter',\n",
        "         4: 'Autumn'}\n",
        "dfn['season_enc'] = dfn['season'].map(dict2)"
      ],
      "metadata": {
        "id": "rxpmB3jyz34m"
      },
      "id": "rxpmB3jyz34m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will be comparing two different categorical variables, 'season' and 'weather'. So will perform a **Chi-square test**."
      ],
      "metadata": {
        "id": "oNSETDmy1Myv"
      },
      "id": "oNSETDmy1Myv"
    },
    {
      "cell_type": "code",
      "source": [
        "contigency= pd.crosstab(dfn.season_enc, dfn.weather_enc)\n",
        "contigency"
      ],
      "metadata": {
        "id": "TUdiwEs40vOL"
      },
      "id": "TUdiwEs40vOL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contigency.plot(kind='bar')"
      ],
      "metadata": {
        "id": "ziL0jH6g2Hwa"
      },
      "id": "ziL0jH6g2Hwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3:** Decide the significance level\n",
        "\n",
        "The significance level (Î±) is already set to 5% i.e., 0.05"
      ],
      "metadata": {
        "id": "HSpuLFg51n5z"
      },
      "id": "HSpuLFg51n5z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4:** Calculate the p-value"
      ],
      "metadata": {
        "id": "hVqApkVB2s56"
      },
      "id": "hVqApkVB2s56"
    },
    {
      "cell_type": "code",
      "source": [
        "chi2, pval, dof, exp_freq = chi2_contingency(contigency, correction=False) #HW - read about correction param - and the history behind it\n",
        "print('Chi-square Statistic: {} \\n P-value: {} \\n Degree of Freedom: {} \\n Expected Frequencies: {}'.format(chi2, pval, dof, exp_freq))"
      ],
      "metadata": {
        "id": "OMb37Jx61oiT"
      },
      "id": "OMb37Jx61oiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result(pval, alpha)"
      ],
      "metadata": {
        "id": "QzWP8tA53AaU"
      },
      "id": "QzWP8tA53AaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Since the p-value is less than the 5% significance level, we reject the null hypothesis. Hence, we have enough statistical evidence to say that the weather conditions are dependent on the ongoing season."
      ],
      "metadata": {
        "id": "iK_PV2gN3p4k"
      },
      "id": "iK_PV2gN3p4k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights and Recommendations"
      ],
      "metadata": {
        "id": "ppdVuxg_-n4g"
      },
      "id": "ppdVuxg_-n4g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **EDA based insights -**\n",
        "\n",
        "1. Total 10,886 rows were present in the data set.\n",
        "2. Neither missing values, nor duplicate rows were found.\n",
        "3. 'temp' and 'atemp' columns were found to be highly correlated. </br> Dropping one of them (atemp) to avoid multicollinearity.\n",
        "4. 'count', 'casual' and 'registered' columns were highly correlated. </br> Dropping casual & registered columns to avoid multicollinearity.\n",
        "5. Outlier values were found in the 'count' column.\n",
        "\n",
        "#### **Insights from hypothesis testing -**\n",
        "1. The no. of bikes rented on weekdays is comparatively higher than on weekends.\n",
        "2. The no. of bikes rented on regular days is comparatively higher than on holidays.\n",
        "2. The demand of bicycles on rent differs under different weather conditions.\n",
        "3. The demand of bicycles on rent is different during different seasons.\n",
        "4. The weather conditions are surely dependent upon the ongoing season.\n",
        "\n",
        "#### **Miscellaneous observations -**\n",
        "The distribution of 'count' column wasn't actually normal or near normal. </br> Infact the column's distribution is found to be a bit skewed towards right.\n",
        "\n",
        "#### **Generic recommendations -**\n",
        "\n",
        "* The demand of bikes on rent are usually higher during Weekdays.\n",
        "* The demand of bikes on rent are usually higher during Regular days.\n",
        "* The chances of person renting a bike are usually higher during Season 3.\n",
        "* The chances of person renting a bike are usually higher during Weather condition 1.\n",
        "\n",
        "We recommend the company to maintain the bike stocks accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "r2MPKT9d-tDG"
      },
      "id": "r2MPKT9d-tDG"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdiRKX7OXy1p"
      },
      "id": "AdiRKX7OXy1p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}